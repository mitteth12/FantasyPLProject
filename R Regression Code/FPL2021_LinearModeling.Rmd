---
title: "FPL Regression Analysis"
author: "Ethan Mitten"
date: "10/11/2021"
output: html_document
---

#Sources Used: http://www.sthda.com/english/articles/37-model-selection-essentials-in-r/155-best-subsets-regression-essentials-in-r/


```{r}
library(dplyr)
```

```{r}
#Load in Data and Packages
FPL_2021 <- read.csv("~/Desktop/Data Analytics/Fantasy PL Project/FantasyPL_Data/2020-21/players_raw.csv")


#Select Certain Columns to Observe and Make a Model With
FPL2021_Data <- FPL_2021 %>% select(-c(first_name, second_name, corners_and_indirect_freekicks_order,
                                       direct_freekicks_order, dreamteam_count, ict_index_rank, id,
                                       influence_rank, penalties_missed, penalties_order, red_cards, yellow_cards,
                                       team_code, threat_rank, threat_rank_type, creativity_rank))
```

#Analyzing 2020-21 Season Data

```{r}
#Load in Packages
#install.packages("leaps")
library(tidyverse)
library(caret)
library(leaps)
```

```{r}
#Get Visual of 2020-21 Season Data
sample_n(FPL2021_Data, 5)
```

```{r}
#Make Multi Linear Regression Model Based on Variables
MLR2021_model <- lm(total_points~., data = FPL2021_Data)
summary(MLR2021_model)
```
#From our test we can see that our R^2 is 99% which is very high. Either these statistics are very good at predicting total points or we have some statistics that are very close to total_points. Looking back through the data points_per_game might be the statistic that is causing this issue. Let us take this variable out and see how the model performs.

```{r}
#Make new dataframe without points per game and run model again
FPL2021_minusppg <- FPL2021_Data %>% select(-points_per_game)
MLR2021_revmodel <- lm(total_points~., data = FPL2021_minusppg)
summary(MLR2021_revmodel)
```
#Interestingly enough from our p-values the variables that are still pulling this to a really high R^2 value are assists, bonus, bps, clean_sheets, goals_scored, minutes and saves none of which are direct indicators by themselves in a player's total points. From here lets find the best model

```{r}
#Finding Best Model Using Best Subset Method
model_2021 <- regsubsets(total_points~., data = FPL2021_minusppg, nvmax=16)
summary(model_2021)
```

#The above shows what the best variables for each amount of variables are. For example, the best 1-variable predictor model is "influence" and the best 2-variable predictor model is "bps" and "threat". We are going to go from here to decide which model does the best job.


```{r}
#See which model works best for adj r^2, cp, and bic
res.sum <- summary(model_2021)
data.frame(
  Adj.R2 = which.max(res.sum$adjr2),
  CP = which.min(res.sum$cp),
  BIC = which.min(res.sum$bic)
)
```

#Using the above test it can be determined that the best adjusted R^2 is found with the model with 14 variables, but the best CP and BIC are found with the model having 9 variables. To prevent overfitting we are going to use another method to ensure the best model selection.

#K-Fold Cross Validation

```{r}
#k-folds formula taken from above source
# id: model id
# object: regsubsets object
# data: data used to fit regsubsets
# outcome: outcome variable
get_model_formula <- function(id, object, outcome){
  # get models data
  models <- summary(object)$which[id,-1]
  # Get outcome variable
  #form <- as.formula(object$call[[2]])
  #outcome <- all.vars(form)[1]
  # Get model predictors
  predictors <- names(which(models == TRUE))
  predictors <- paste(predictors, collapse = "+")
  # Build model formula
  as.formula(paste0(outcome, "~", predictors))
}
```

```{r}
#Function to get the cross validation error for a model
get_cv_error <- function(model.formula, data){
  set.seed(1)
  train.control <- trainControl(method = "cv", number = 14)
  cv <- train(model.formula, data = data, method = "lm",
              trControl = train.control)
  cv$results$RMSE
}
```

```{r}
# Compute cross-validation error
model.ids <- 1:14
cv.errors <-  map(model.ids, get_model_formula, model_2021, "total_points") %>%
  map(get_cv_error, data = FPL2021_minusppg) %>%
  unlist()
cv.errors
```
```{r}
# Select the model that minimize the CV error
which.min(cv.errors)
```

```{r}
#Best Predicting Model
coef(model_2021, 9)
```

#Using k-folds confirms our variables that seem to be the best predictors of total_points.

````{r}
best2021_model <- lm(total_points~assists + bonus + bps + clean_sheets + goals_scored + influence + minutes + saves + threat, data=FPL2021_minusppg)
```

#Standard Residual
#Measure of the strength of the difference between observed and expected values
```{r}
data.resid <- resid(best2021_model)
```

#Studentized Residual
#Residual divided by the individualized(estimated) standard deviation
```{r}
data.stresid <- rstandard(best2021_model)
```

#Deleted t Residual (Preferred Method)
#Residual divided by individualized (estimated) standard deviation based on data excluding that point
```{r}
data.rstudent <- rstudent(best2021_model)
```

#Predicted total points for each player in the FPL2021_minusppg dataset
```{r}
predictz <- predict(best2021_model)
predictz
```

#Plots comparing the predicted values to the studentized and standard residuals
```{r}
plot(predictz, data.stresid)
plot(predictz, data.resid)
```
#If the model fits well there should be no patterns present for either of the predictors and as can be seen by the plots there does not seem to be any distinct trends

#Normal Q-Q Plot
```{r}
qqnorm(data.rstudent, pch=20)
qqline(data.rstudent, col="blue")
```

#In the qqplot we are looking for the data points to follow the qqplot line and to be almost a straight line, but we see we run into some trouble making that conclusion. This plot indicates that our random values DO NOT follow a normal distribution. 

#Some reasons for this could be that:
1. We have some extreme values in our data
2. Insufficient Data Discrimination
3. Overlapping processes
4. Values Close to Zero or Natural Limit

Source: https://www.isixsigma.com/tools-templates/normality/dealing-non-normal-data-strategies-and-tools/

#Histogram of deleted t residuals
```{r}
hist(data.rstudent) #histogram
```

#When looking at a histogram of the deleted t residuals we would like to see a normal distribution in the histogram. It is a little tough to make a conclusion as it roughly follows a normal distribution pattern, but we have a higher frequency of data on the right side of our middle bar than the left side.

#Do residual versus explanatory variables
```{r}
plot(FPL2021_minusppg$assists, data.rstudent, pch=20) #residual vs. assists
abline(h=0)
plot(FPL2021_minusppg$bonus, data.rstudent, pch=20) #residual vs. bonus
abline(h=0)
plot(FPL2021_minusppg$bps, data.rstudent, pch=20) #residual vs. bps
abline(h=0)
plot(FPL2021_minusppg$clean_sheets, data.rstudent, pch=20) #residual vs. clean_sheets
abline(h=0)
plot(FPL2021_minusppg$goals_scored, data.rstudent, pch=20) #residual vs. goals_scored
abline(h=0)
plot(FPL2021_minusppg$influence, data.rstudent, pch=20) #residual vs. influence
abline(h=0)
plot(FPL2021_minusppg$minutes, data.rstudent, pch=20) #residual vs. minutes
abline(h=0)
plot(FPL2021_minusppg$saves, data.rstudent, pch=20) #residual vs. saves
abline(h=0)
plot(FPL2021_minusppg$threat, data.rstudent, pch=20) #residual vs. threat
abline(h=0)
```
#In these graphs of residual vs. explanatory variable we are looking for variables that are random in the graph to suggest a good fit. When looking at the data it might be deemed that saves is not a variable that would be considered random, but all the rest of the values fall pretty well in line.


#Added Variable Plots
```{r}
library(car)
avPlots(best2021_model)
```

#In this we are looking to judge the marginal usefulness of each explanatory variable. We want the data points to follow the lines. Almost all of these graphs has outlying players invovled in the process. One player that looks to be present in about every graphic is 321. Let us take a look at what is going on with this player.

```{r}
#Roberto Firmino
FPL_2021[321,]
```

#Firmino seems to be a player that stats would lead him to have a higher total number of points than he did. Lets verify this by comparing his 141 points that he scored to what was predicted for him

```{r}
#Predicted Firmino Points
predictz[321]
```
#Firmino's predicted points were off by 24 points which is not the huge residual I thought it would be. Still is a big difference compared to what we are seeing from our normal distribution which is showing that 99% of our data is within the range of -4 to 4 for the residual. 






#The Next Step

#There seem to be some problems that are going on with our model in the form of some assumptions that are not being met. Sitting back and thinking about what these problems could be I have come to a conclusion that we may need to make models different for each position. If I am making a model predicting total points and saves is shown to be significant than that is problematic. This is because a heavy majority of the players in the dataset are going to have 0 saves because most of them are not goalkeepers or are goalkeepers that did not play. This is backed by the scatterplot presented earlier looking at saves. We should divide this dataset of 600 into each position and go through the same process and see if we are getting a normal distribution.

```{r}
NewFPL2021 <- read_csv("~/Desktop/Data Analytics/Fantasy PL Project/FantasyPL_Data/2020-21/players_raw.csv")
```














